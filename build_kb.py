# build_kb.py
import json, re, os, uuid
from bs4 import BeautifulSoup

DATA_DIR    = "./data"
OUTPUT_FILE = "chunks.json"
CHUNK_SIZE  = 250   # ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª Ù‡Ø± chunk
OVERLAP     = 50    # Ù‡Ù…Ù¾ÙˆØ´Ø§Ù†ÛŒ Ø¨ÛŒÙ† chunkâ€ŒÙ‡Ø§

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ù†Ú¯Ø§Ø´Øª Ù†Ø§Ù… ÙØ§ÛŒÙ„ â†’ Ù†Ø§Ù… ÙØ§Ø±Ø³ÛŒ Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
FILE_NAMES = {
    "ai_etiquette.html":        "Ø¢Ø¯Ø§Ø¨â€ŒÙ†Ø§Ù…Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§Ø¨Ø²Ø§Ø± Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ",
    "undergraduate_new.html":   "Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø¯ÙˆØ±Ù‡ Ú©Ø§Ø±Ø´Ù†Ø§Ø³ÛŒ (ÙˆØ±ÙˆØ¯ÛŒ Û±Û´Û°Û² Ùˆ Ø¨Ø¹Ø¯)",
    "undergraduate_old.html":   "Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø¯ÙˆØ±Ù‡ Ú©Ø§Ø±Ø´Ù†Ø§Ø³ÛŒ (ÙˆØ±ÙˆØ¯ÛŒ Û±Û´Û°Û± Ùˆ Ù…Ø§Ù‚Ø¨Ù„)",
    "article_submission.html":  "Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡ Ø§Ø±Ø³Ø§Ù„ Ù…Ù‚Ø§Ù„Ù‡ Ø¨Ù‡ Ù…Ø¬Ù„Ø§Øª Ø¹Ù„Ù…ÛŒ Ùˆ Ù‡Ù…Ø§ÛŒØ´â€ŒÙ‡Ø§",
    "exam_regulations.html":    "Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡ Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒ Ùˆ ØºÛŒØ¨Øª Ø¯Ø± Ø§Ù…ØªØ­Ø§Ù†Ø§Øª",
    "double_major.html":        "Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡ ØªØ­ØµÛŒÙ„ Ù‡Ù…Ø²Ù…Ø§Ù† Ø¯Ø± Ø¯Ùˆ Ø±Ø´ØªÙ‡ (Ø§Ø³ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø±Ø®Ø´Ø§Ù†)",
    "teaching_assistant.html":  "Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡ Ø¯Ø³ØªÛŒØ§Ø±ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ",
    "coop.html":                "Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡ Ø¯ÙˆØ±Ù‡ Ú©Ø§Ø± Ùˆ Ø¢Ù…ÙˆØ²Ø´ Ù¾Ø§ÛŒØ¯Ø§Ø± (Ú©ÙˆØ¢Ù¾)",
    "minor.html":               "Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡ Ø¯ÙˆØ±Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ±Ø¹ÛŒ Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ØµÙ†Ø¹ØªÛŒ Ø´Ø±ÛŒÙ",
    "prerequisites.html":       "Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡ Ø±ÙˆØ§Ø¨Ø· Ù¾ÛŒØ´â€ŒÙ†ÛŒØ§Ø²ÛŒ Ùˆ Ù‡Ù…Ù†ÛŒØ§Ø²ÛŒ",
    "internship.html":          "Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡ Ú©Ø§Ø±Ø¢Ù…ÙˆØ²ÛŒ",
    "internship_grade.html":    "Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„ Ø§Ø®Ø° Ùˆ Ø§Ø±Ø§Ø¦Ù‡ Ù†Ù…Ø±Ù‡ Ø¯Ø±Ø³ Ú©Ø§Ø±Ø¢Ù…ÙˆØ²ÛŒ",
    "online_courses.html":      "Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ø±ÙˆØ³ Ø¨Ù‡ ØµÙˆØ±Øª ØºÛŒØ±Ø­Ø¶ÙˆØ±ÛŒ",
    "medical_absence.html":     "Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„ Ø¨Ø±Ø±Ø³ÛŒ Ù…ÙˆØ§Ø±Ø¯ ØºÛŒØ¨Øª Ù¾Ø²Ø´Ú©ÛŒ Ø¯Ø± Ø§Ù…ØªØ­Ø§Ù† Ù¾Ø§ÛŒØ§Ù†â€ŒØªØ±Ù…",
    "bachelor_project.html":    "Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„ Ø«Ø¨Øªâ€ŒÙ†Ø§Ù… Ùˆ Ø§Ø±Ø§Ø¦Ù‡ Ù†Ù…Ø±Ù‡ Ø¯Ø±Ø³ Ù¾Ø±ÙˆÚ˜Ù‡ Ú©Ø§Ø±Ø´Ù†Ø§Ø³ÛŒ",
    "professor_referral.html":  "Ø´Ø±Ø§ÛŒØ· Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¹Ø±ÙÛŒ Ø¨Ù‡ Ø§Ø³ØªØ§Ø¯",
    "guest_external.html":      "Ø´Ø±Ø§ÛŒØ· Ù…Ù‡Ù…Ø§Ù†ÛŒ Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù† Ø´Ø±ÛŒÙ Ø¯Ø± Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡â€ŒÙ‡Ø§ÛŒ Ø¯ÙˆÙ„ØªÛŒ",
    "saipa_scholarship.html":   "Ø´Ø±Ø§ÛŒØ· Ùˆ Ø¶ÙˆØ§Ø¨Ø· Ù¾Ø°ÛŒØ±Ø´ Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒ Ø¨ÙˆØ±Ø³ÛŒÙ‡ Ú¯Ø±ÙˆÙ‡ Ø®ÙˆØ¯Ø±ÙˆØ³Ø§Ø²ÛŒ Ø³Ø§ÛŒÙ¾Ø§",
    "transfer.html":            "Ø´ÛŒÙˆÙ‡â€ŒÙ†Ø§Ù…Ù‡ Ø§Ù†ØªÙ‚Ø§Ù„ Ø¨Ù‡ Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ØµÙ†Ø¹ØªÛŒ Ø´Ø±ÛŒÙ",
    "guest_internal.html":      "Ø´ÛŒÙˆÙ‡â€ŒÙ†Ø§Ù…Ù‡ Ù…Ù‡Ù…Ø§Ù†ÛŒ Ø¯Ø± Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ØµÙ†Ø¹ØªÛŒ Ø´Ø±ÛŒÙ",
    "course_equivalency.html":  "Ø´ÛŒÙˆÙ‡â€ŒÙ†Ø§Ù…Ù‡ Ù¾Ø°ÛŒØ±Ø´ Ùˆ ØªØ·Ø¨ÛŒÙ‚ Ø¯Ø±ÙˆØ³ Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù† Ù…Ù‡Ù…Ø§Ù†ØŒ Ø§Ù†ØªÙ‚Ø§Ù„ÛŒØŒ Ø§Ù†ØµØ±Ø§ÙÛŒ Ùˆ ØªØºÛŒÛŒØ± Ø±Ø´ØªÙ‡",
    "change_major.html":        "Ù‚ÙˆØ§Ù†ÛŒÙ† ØªØºÛŒÛŒØ± Ø±Ø´ØªÙ‡ Ø¯Ø± Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø¯ÙˆØ±Ù‡ Ú©Ø§Ø±Ø´Ù†Ø§Ø³ÛŒ",
    "military_exemption.html":  "Ù‚ÙˆØ§Ù†ÛŒÙ† Ùˆ Ù…Ù‚Ø±Ø±Ø§Øª Ù…Ø¹Ø§ÙÛŒØª ØªØ­ØµÛŒÙ„ÛŒ Ùˆ Ù…Ø´ÙˆÙ‚â€ŒÙ‡Ø§ÛŒ Ø®Ø¯Ù…ØªÛŒ",
    "olympiad_cr.html":         "Ù„ÛŒØ³Øª Ø¯Ø±ÙˆØ³ Ù‚Ø§Ø¨Ù„ Ù¾Ø°ÛŒØ±Ø´ (CR) Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù† Ù…Ø¯Ø§Ù„â€ŒØ¢ÙˆØ± Ø§Ù„Ù…Ù¾ÛŒØ§Ø¯",
    "graduation_deadline.html": "Ù…Ù‡Ù„Øª Ø§Ù†Ø¬Ø§Ù… Ø§Ù…ÙˆØ± ÙØ±Ø§ØºØª Ø§Ø² ØªØ­ØµÛŒÙ„",
    "academic_charter.html":    "Ù†Ø¸Ø§Ù…â€ŒÙ†Ø§Ù…Ù‡ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ØµÙ†Ø¹ØªÛŒ Ø´Ø±ÛŒÙ",
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø³Ø§Ø®ØªØ§Ø± Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
STRUCTURAL_PATTERN = re.compile(
    r'^(Ù…Ø§Ø¯Ù‡\s*\d+|Ø¨Ù†Ø¯\s*\d+|ØªØ¨ØµØ±Ù‡\s*\d*|ÙØµÙ„\s*\d+|'
    r'Ø¨Ø®Ø´\s*\d+|Ø§Ù„Ù[â€Œ\s]|Ø¨[â€Œ\s]|Ø¬[â€Œ\s]|Ø¯[â€Œ\s])'
)

# Ø®Ø·ÙˆØ·ÛŒ Ú©Ù‡ Ø¨Ø§ÛŒØ¯ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´ÙˆÙ†Ø¯ (Ù‡Ø¯Ø±/ÙÙˆØªØ±/Ù…Ù†Ùˆ)
SKIP_PATTERNS = re.compile(
    r'^(Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ØµÙ†Ø¹ØªÛŒ Ø´Ø±ÛŒÙ|Ù…Ø¯ÛŒØ±ÛŒØª Ø§Ù…ÙˆØ± Ø¢Ù…ÙˆØ²Ø´ÛŒ|ØµÙØ­Ù‡\s*\d+|'
    r'Ú†Ø§Ù¾|Ø¨Ø§Ø²Ú¯Ø´Øª|Ù…Ù†Ùˆ|ÙˆØ±ÙˆØ¯|Ø®Ø±ÙˆØ¬|Ø¬Ø³ØªØ¬Ùˆ|Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡â€ŒÙ‡Ø§ Ùˆ Ù…Ù‚Ø±Ø±Ø§Øª'
    r'|ØªÙ…Ø§Ù…ÛŒ Ø­Ù‚ÙˆÙ‚|Â©|https?://|www\.).*$',
    re.IGNORECASE
)


def normalize(text: str) -> str:
    """Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ"""
    # ÛŒÚ©Ø³Ø§Ù†â€ŒØ³Ø§Ø²ÛŒ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø¹Ø±Ø¨ÛŒ/ÙØ§Ø±Ø³ÛŒ
    text = text.replace('Ùƒ', 'Ú©').replace('ÙŠ', 'ÛŒ')
    text = text.replace('\u200c', ' ')   # Ù†ÛŒÙ…â€ŒÙØ§ØµÙ„Ù‡ â†’ ÙØ§ØµÙ„Ù‡
    text = re.sub(r'\s+', ' ', text)     # ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ú†Ù†Ø¯Ú¯Ø§Ù†Ù‡
    text = re.sub(r'[_\-]{3,}', '', text)  # Ø®Ø·â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯Ø§Ú©Ù†Ù†Ø¯Ù‡
    return text.strip()


def is_skip_line(line: str) -> bool:
    """Ø¢ÛŒØ§ Ø®Ø· Ø¨Ø§ÛŒØ¯ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´ÙˆØ¯ØŸ"""
    if len(line) < 3:
        return True
    if SKIP_PATTERNS.match(line):
        return True
    # Ø®Ø·ÙˆØ· Ø¹Ø¯Ø¯ÛŒ ØµØ±Ù (Ø´Ù…Ø§Ø±Ù‡ ØµÙØ­Ù‡)
    if re.match(r'^\d{1,3}$', line):
        return True
    return False


def make_chunk(text: str, reg_name: str, article_num: str = "") -> dict:
    return {
        "id": str(uuid.uuid4()),
        "regulation_name": reg_name,
        "article_number": article_num,
        "text": normalize(text),
    }


def split_long_text(text: str, reg_name: str, article_num: str,
                    size: int = CHUNK_SIZE, overlap: int = OVERLAP) -> list:
    """ØªÙ‚Ø³ÛŒÙ… Ù…ØªÙ† Ø¨Ù„Ù†Ø¯ Ø¨Ø§ Ù¾Ù†Ø¬Ø±Ù‡ Ù„ØºØ²Ù†Ø¯Ù‡"""
    words = text.split()
    if len(words) <= size:
        t = normalize(text)
        return [make_chunk(t, reg_name, article_num)] if len(t) > 20 else []

    chunks = []
    start = 0
    while start < len(words):
        end = min(start + size, len(words))
        chunk_text = normalize(' '.join(words[start:end]))
        if len(chunk_text) > 20:
            chunks.append(make_chunk(chunk_text, reg_name, article_num))
        if end == len(words):
            break
        start += size - overlap
    return chunks


def parse_html(filepath: str, reg_name: str) -> list:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ùˆ chunking ÛŒÚ© ÙØ§ÛŒÙ„ HTML"""
    with open(filepath, 'r', encoding='utf-8') as f:
        soup = BeautifulSoup(f.read(), 'html.parser')

    # Ø­Ø°Ù Ø¹Ù†Ø§ØµØ± ØºÛŒØ±Ù…ØªÙ†ÛŒ
    for tag in soup.find_all(['header', 'footer', 'nav',
                               'script', 'style', 'noscript',
                               'iframe', 'button', 'form']):
        tag.decompose()

    # ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ÛŒ Ø§ØµÙ„ÛŒ
    main_content = (
        soup.find('main') or
        soup.find('article') or
        soup.find('div', class_=re.compile(r'content|main|body', re.I)) or
        soup.find('body') or
        soup
    )

    lines = [
        l.strip()
        for l in main_content.get_text('\n').split('\n')
        if l.strip() and not is_skip_line(l.strip())
    ]

    all_chunks = []
    current_article = ""
    current_buf = []

    def flush():
        """Ø°Ø®ÛŒØ±Ù‡ buffer ÙØ¹Ù„ÛŒ Ø¨Ù‡â€ŒØ¹Ù†ÙˆØ§Ù† chunk"""
        if current_buf:
            joined = ' '.join(current_buf)
            all_chunks.extend(
                split_long_text(joined, reg_name, current_article)
            )

    for line in lines:
        is_structural = STRUCTURAL_PATTERN.match(line)

        if is_structural:
            flush()
            current_article = is_structural.group(0).strip()
            current_buf = [line]
        else:
            current_buf.append(line)
            # Ø§Ú¯Ø± buffer Ø®ÛŒÙ„ÛŒ Ø¨Ø²Ø±Ú¯ Ø´Ø¯ØŒ flush Ú©Ù†
            if len(' '.join(current_buf).split()) >= CHUNK_SIZE:
                flush()
                current_buf = []

    flush()  # Ø¢Ø®Ø±ÛŒÙ† buffer
    return all_chunks


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ø§Ø¬Ø±Ø§ÛŒ Ø§ØµÙ„ÛŒ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    all_chunks = []
    missing = []

    for fname, rname in FILE_NAMES.items():
        fp = os.path.join(DATA_DIR, fname)
        if not os.path.exists(fp):
            missing.append(fname)
            print(f"  [SKIP] ÙØ§ÛŒÙ„ ÛŒØ§ÙØª Ù†Ø´Ø¯: {fname}")
            continue

        chs = parse_html(fp, rname)
        all_chunks.extend(chs)
        print(f"  âœ… {rname[:40]:<40} â†’ {len(chs):>4} chunk")

    print(f"\n{'â”€'*60}")
    print(f"ğŸ“¦ Ù…Ø¬Ù…ÙˆØ¹ chunks: {len(all_chunks)}")
    if missing:
        print(f"âš ï¸  ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ú¯Ù…â€ŒØ´Ø¯Ù‡ ({len(missing)}): {', '.join(missing)}")

    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(all_chunks, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {OUTPUT_FILE}")

    # Ø¢Ù…Ø§Ø±
    print(f"\nğŸ“Š Ø¢Ù…Ø§Ø± chunks Ø¨Ù‡ ØªÙÚ©ÛŒÚ© Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡:")
    from collections import Counter
    counts = Counter(c['regulation_name'] for c in all_chunks)
    for name, cnt in sorted(counts.items(), key=lambda x: -x[1]):
        print(f"   {cnt:>4}  {name}")


if __name__ == "__main__":
    main()
