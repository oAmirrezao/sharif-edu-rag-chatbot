# chatbot.py â€” Ù†Ø³Ø®Ù‡ Ù†Ù‡Ø§ÛŒÛŒ
import pickle
import numpy as np
import faiss
import gradio as gr
from sentence_transformers import SentenceTransformer
from groq import Groq

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GROQ_API_KEY = "--"
LLM_MODEL    = "qwen-qwq-32b"
EMBED_MODEL  = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
INDEX_FILE   = "faiss.index"
META_FILE    = "chunks_meta.pkl"
TOP_K        = 7
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("ğŸ“¥ Embedding model (CPU)...")
embed_model = SentenceTransformer(EMBED_MODEL, device='cpu')

print("ğŸ“‚ FAISS index...")
index = faiss.read_index(INDEX_FILE)
with open(META_FILE, 'rb') as f:
    chunks = pickle.load(f)

print("ğŸ”— Ø§ØªØµØ§Ù„ Ø¨Ù‡ Groq API...")
client = Groq(api_key=GROQ_API_KEY)
print(f"âœ… Ø¢Ù…Ø§Ø¯Ù‡! (Ù…Ø¯Ù„: {LLM_MODEL})")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def retrieve(question: str, top_k: int = TOP_K) -> list:
    q_emb = embed_model.encode(
        [question], normalize_embeddings=True
    ).astype(np.float32)
    scores, idxs = index.search(q_emb, top_k)
    return [
        {"chunk": chunks[i], "score": float(s)}
        for s, i in zip(scores[0], idxs[0]) if i < len(chunks)
    ]


def build_context(retrieved: list) -> str:
    parts = []
    for item in retrieved:
        c = item['chunk']
        ref = c['regulation_name']
        if c.get('article_number'):
            ref += f"ØŒ {c['article_number']}"
        parts.append(f"[{ref}]\n{c['text']}")
    return "\n\n---\n\n".join(parts)


def answer(question: str) -> tuple:
    if not question.strip():
        return "Ø³ÙˆØ§Ù„ ÙˆØ§Ø±Ø¯ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª.", ""

    retrieved = retrieve(question)
    context   = build_context(retrieved)

    try:
        response = client.chat.completions.create(
            model=LLM_MODEL,
            messages=[
                {
                    "role": "system",
                    "content": (
                        "Ø´Ù…Ø§ Ø¯Ø³ØªÛŒØ§Ø± Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ØµÙ†Ø¹ØªÛŒ Ø´Ø±ÛŒÙ Ù‡Ø³ØªÛŒØ¯.\n"
                        "Ù‚ÙˆØ§Ù†ÛŒÙ† Ù¾Ø§Ø³Ø®â€ŒØ¯Ù‡ÛŒ:\n"
                        "Û±. ÙÙ‚Ø· Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…ØªÙˆÙ† Ø§Ø±Ø§Ø¦Ù‡â€ŒØ´Ø¯Ù‡ Ù¾Ø§Ø³Ø® Ø¯Ù‡ÛŒØ¯.\n"
                        "Û². Ø´Ù…Ø§Ø±Ù‡ Ù…Ø§Ø¯Ù‡ØŒ Ø¨Ù†Ø¯ ÛŒØ§ ØªØ¨ØµØ±Ù‡ Ø±Ø§ Ø­ØªÙ…Ø§Ù‹ Ø°Ú©Ø± Ú©Ù†ÛŒØ¯.\n"
                        "Û³. Ù‡ÛŒÚ† Ù‚Ø§Ù†ÙˆÙ† ÛŒØ§ ØªÙØ³ÛŒØ±ÛŒ Ø®Ø§Ø±Ø¬ Ø§Ø² Ù…ØªÙ† Ù†Ø³Ø§Ø²ÛŒØ¯.\n"
                        "Û´. Ø§Ú¯Ø± Ù¾Ø§Ø³Ø® Ø¯Ø± Ù…ØªÙˆÙ† Ù†Ø¨ÙˆØ¯ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯: Â«Ø¯Ø± Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡ Ù…ÙˆØ¬ÙˆØ¯ Ù…Ø·Ù„Ø¨ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ø² Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø±Ø¯.Â»\n"
                        "Ûµ. Ù¾Ø§Ø³Ø® Ú©ÙˆØªØ§Ù‡ØŒ Ø¯Ù‚ÛŒÙ‚ Ùˆ Ù…Ø³ØªÙ†Ø¯ Ø¨Ø§Ø´Ø¯."
                    )
                },
                {
                    "role": "user",
                    "content": (
                        f"Ù…ØªÙˆÙ† Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡:\n\n{context}\n\n"
                        f"{'â”€'*40}\n\n"
                        f"Ø³ÙˆØ§Ù„ Ø¯Ø§Ù†Ø´Ø¬Ùˆ: {question}\n\n"
                        f"Ù¾Ø§Ø³Ø® (Ø¨Ø§ Ø°Ú©Ø± Ø´Ù…Ø§Ø±Ù‡ Ù…Ø§Ø¯Ù‡/Ø¨Ù†Ø¯):"
                    )
                }
            ],
            max_tokens=800,
            temperature=0.1,
        )
        ans = response.choices[0].message.content.strip()
    except Exception as e:
        ans = f"âŒ Ø®Ø·Ø§ Ø¯Ø± API: {str(e)}"

    src = "### ğŸ“š Ù…Ù†Ø§Ø¨Ø¹ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒâ€ŒØ´Ø¯Ù‡\n"
    for i, item in enumerate(retrieved, 1):
        c = item['chunk']
        src += f"\n**{i}.** {c['regulation_name']}"
        if c.get('article_number'):
            src += f" â€” {c['article_number']}"
        src += f" *(score: {item['score']:.3f})*\n"
        src += f"> {c['text'][:120]}...\n"

    return ans, src


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def chat(question: str, history: list) -> tuple:
    if not question.strip():
        return history or [], history or [], "*Ù…Ù†Ø§Ø¨Ø¹ Ø§ÛŒÙ†Ø¬Ø§ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.*", ""
    ans, src = answer(question)
    history = history or []
    history.append({"role": "user",      "content": question})
    history.append({"role": "assistant", "content": ans})
    return history, history, src, ""


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with gr.Blocks(title="Ú†Øªâ€ŒØ¨Ø§Øª Ø´Ø±ÛŒÙ") as demo:
    gr.Markdown("# ğŸ“ Ú†Øªâ€ŒØ¨Ø§Øª Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ØµÙ†Ø¹ØªÛŒ Ø´Ø±ÛŒÙ")
    gr.Markdown(f"*Ù…Ø¯Ù„: {LLM_MODEL} via Groq API*")

    with gr.Row():
        with gr.Column(scale=2):
            chatbot_ui = gr.Chatbot(
                height=480,
                type="messages"
            )
            with gr.Row():
                q_input = gr.Textbox(
                    placeholder="Ø³ÙˆØ§Ù„ Ø®ÙˆØ¯ Ø±Ø§ Ø§ÛŒÙ†Ø¬Ø§ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯...",
                    label="",
                    scale=4
                )
                send_btn = gr.Button("Ø§Ø±Ø³Ø§Ù„", variant="primary", scale=1)
            clear_btn = gr.Button("ğŸ—‘ Ù¾Ø§Ú© Ú©Ø±Ø¯Ù†")

        with gr.Column(scale=1):
            sources_md = gr.Markdown("*Ù…Ù†Ø§Ø¨Ø¹ Ø§ÛŒÙ†Ø¬Ø§ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.*")

    gr.Examples(
        label="Ù†Ù…ÙˆÙ†Ù‡ Ø³ÙˆØ§Ù„Ø§Øª",
        examples=[
            "Ø­Ø¯Ø§Ú©Ø«Ø± Ø³Ù†ÙˆØ§Øª Ù…Ø¬Ø§Ø² ØªØ­ØµÛŒÙ„ Ø¯Ø± Ø¯ÙˆØ±Ù‡ Ú©Ø§Ø±Ø´Ù†Ø§Ø³ÛŒ Ú†Ù‚Ø¯Ø± Ø§Ø³ØªØŸ",
            "Ø´Ø±Ø§ÛŒØ· Ù…Ø±Ø®ØµÛŒ ØªØ­ØµÛŒÙ„ÛŒ Ú†ÛŒØ³ØªØŸ",
            "Ø¯Ø± ØµÙˆØ±Øª ØºÛŒØ¨Øª Ø¯Ø± Ø§Ù…ØªØ­Ø§Ù† Ù¾Ø§ÛŒØ§Ù†â€ŒØªØ±Ù… Ú†Ù‡ Ù†Ù…Ø±Ù‡â€ŒØ§ÛŒ Ø«Ø¨Øª Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŸ",
            "Ø¢ÛŒØ§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù… Ø¨ÛŒØ´ Ø§Ø² Û²Û´ ÙˆØ§Ø­Ø¯ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†Ù…ØŸ",
            "Ø´Ø±Ø§ÛŒØ· Ø§Ù†ØªÙ‚Ø§Ù„ Ø¨Ù‡ Ø´Ø±ÛŒÙ Ú†ÛŒØ³ØªØŸ",
            "Ø´Ø±Ø§ÛŒØ· Ø¯ÙˆØ±Ù‡ Ú©ÙˆØ¢Ù¾ Ú†ÛŒØ³ØªØŸ",
            "Ù…Ø¹Ø¯Ù„ Ù„Ø§Ø²Ù… Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù…Ø´Ø±ÙˆØ·ÛŒ Ú†Ù‚Ø¯Ø± Ø§Ø³ØªØŸ",
        ],
        inputs=q_input
    )

    state = gr.State([])
    send_btn.click(chat, [q_input, state], [chatbot_ui, state, sources_md, q_input])
    q_input.submit(chat, [q_input, state], [chatbot_ui, state, sources_md, q_input])
    clear_btn.click(
        lambda: ([], [], "*Ù…Ù†Ø§Ø¨Ø¹ Ø§ÛŒÙ†Ø¬Ø§ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.*"),
        outputs=[chatbot_ui, state, sources_md]
    )

if __name__ == "__main__":
    demo.launch(server_port=7860, share=False)
